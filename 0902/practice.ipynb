{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class PatchEmbed3D(nn.Module):\n",
    "    def __init__(self,\n",
    "                 patch_size = (2, 4, 4),\n",
    "                 in_chans = 3,\n",
    "                 embed_dim = 96,\n",
    "                 norm_layer = None):\n",
    "        super().__init__()\n",
    "\n",
    "        self.patch_size = patch_size\n",
    "        self.in_chans = in_chans\n",
    "        self.embed_dim = embed_dim\n",
    "\n",
    "        self.proj = nn.Conv3d(in_chans, embed_dim, kernel_size=patch_size, stride=patch_size)\n",
    "        if norm_layer is not None:\n",
    "            self.norm = norm_layer(embed_dim)\n",
    "        else:\n",
    "            self.norm = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        # padding\n",
    "        _, _, D, H, W = x.size()\n",
    "        if W % self.patch_size[2] != 0:\n",
    "            x = F.pad(x, (0, self.patch_size[2] - W % self.patch_size[2]))\n",
    "\n",
    "        if H % self.patch_size[1] != 0:\n",
    "            x = F.pad(x, (0, 0, 0, self.patch_size[1] - H % self.patch_size[1]))\n",
    "\n",
    "        if D % self.patch_size[0] != 0:\n",
    "            x = F.pad(x, (0, 0, 0, 0, 0, self.patch_size[0] - D % self.patch_size[0]))\n",
    "\n",
    "        x = self.proj(x)\n",
    "\n",
    "        if self.norm is not None:\n",
    "            D, Wh, Ww = x.size(2), x.size(3), x.size(4)\n",
    "            x = x.flatten(2).transpose(1, 2)\n",
    "            x = self.norm(x)\n",
    "            x = x.transpose(1, 2).view(-1, self.embed_dim, D, Wh, Ww)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = torch.randn(2, 1, 96, 384, 384)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = PatchEmbed3D(patch_size=(16, 64, 64),\n",
    "                     in_chans=1,\n",
    "                     embed_dim=96,\n",
    "                     norm_layer=nn.LayerNorm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 96, 6, 6, 6])\n"
     ]
    }
   ],
   "source": [
    "pred = model(sample)\n",
    "print(pred.shape) # D // p, H // p,  W // p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12\n",
      "[0.0, 0.0181818176060915, 0.036363635212183, 0.05454545468091965, 0.072727270424366, 0.09090908616781235, 0.10909091681241989, 0.12727272510528564, 0.1454545557498932, 0.16363637149333954, 0.1818181872367859, 0.20000000298023224]\n"
     ]
    }
   ],
   "source": [
    "drop_path_rate = 0.2\n",
    "depths = [2, 2, 6, 2]\n",
    "print(sum(depths))\n",
    "dpr = [x.item() for x in torch.linspace(0, drop_path_rate, sum(depths))]\n",
    "print(dpr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "B, C, D, H, W = pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_window_size(x_size, window_size, shift_size = None):\n",
    "    use_window_size = list(window_size)\n",
    "    if shift_size is not None:\n",
    "        use_shift_size = list(shift_size)\n",
    "    for i in range(len(x_size)):\n",
    "        if x_size[i] <= window_size[i]:\n",
    "            use_window_size[i] = x_size[i]\n",
    "            if shift_size is not None:\n",
    "                use_shift_size[i] = 0\n",
    "\n",
    "    if shift_size is None:\n",
    "        return tuple(use_window_size)\n",
    "    else:\n",
    "        return tuple(use_window_size), tuple(use_shift_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 7, 7)\n",
      "(0, 3, 3)\n"
     ]
    }
   ],
   "source": [
    "window_size = (1, 7, 7)\n",
    "shift_size = tuple(i // 2 for i in window_size)\n",
    "print(window_size)\n",
    "print(shift_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 6, 6)\n",
      "(0, 0, 0)\n"
     ]
    }
   ],
   "source": [
    "window_size, shift_size = get_window_size((D, H, W), window_size, shift_size)\n",
    "print(window_size)\n",
    "print(shift_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import reduce, lru_cache\n",
    "from operator import mul\n",
    "\n",
    "def window_partition(x, window_size):\n",
    "    B, D, H, W, C = x.shape\n",
    "\n",
    "    x = x.view(B, \n",
    "               D //  window_size[0], window_size[0],\n",
    "               H // window_size[1], window_size[1], \n",
    "               W // window_size[2], window_size[2], C)\n",
    "    \n",
    "    windows = x.permute(0, 1, 3, 5, 2, 4, 6, 7).contiguous().view(-1, reduce(mul, window_size), C)\n",
    "\n",
    "    return windows\n",
    "\n",
    "def compute_mask(D, H, W, window_size, shift_size, device):\n",
    "    img_mask = torch.zeros((1, D, H, W, 1), device=device)\n",
    "    cnt = 0\n",
    "    for d in slice(-window_size[0]), slice(-window_size[0], -shift_size[0]), slice(-shift_size[0],None):\n",
    "        for h in slice(-window_size[1]), slice(-window_size[1], -shift_size[1]), slice(-shift_size[1],None):\n",
    "            for w in slice(-window_size[2]), slice(-window_size[2], -shift_size[2]), slice(-shift_size[2],None):\n",
    "                img_mask[:, d, h, w, :] = cnt\n",
    "                cnt += 1\n",
    "    mask_windows = window_partition(img_mask, window_size)  # nW, ws[0]*ws[1]*ws[2], 1\n",
    "    mask_windows = mask_windows.squeeze(-1)  # nW, ws[0]*ws[1]*ws[2]\n",
    "    attn_mask = mask_windows.unsqueeze(1) - mask_windows.unsqueeze(2)\n",
    "    attn_mask = attn_mask.masked_fill(attn_mask != 0, float(-100.0)).masked_fill(attn_mask == 0, float(0.0))\n",
    "    return attn_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 6, 6, 6, 96])\n",
      "Dp : 6, Hp : 6, Wp : 6\n"
     ]
    }
   ],
   "source": [
    "from einops import rearrange\n",
    "import numpy as np\n",
    "\n",
    "x = rearrange(pred, 'b c d h w -> b d h w c')\n",
    "print(x.shape)\n",
    "\n",
    "Dp = int(np.ceil(D / window_size[0])) * window_size[0]\n",
    "Hp = int(np.ceil(H / window_size[1])) * window_size[1]\n",
    "Wp = int(np.ceil(W / window_size[2])) * window_size[2]\n",
    "print(f\"Dp : {Dp}, Hp : {Hp}, Wp : {Wp}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "attn_mask = compute_mask(Dp, Wp, Hp, window_size, shift_size, x.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "window_size, shift_size = get_window_size((D, H, W),\n",
    "                                            window_size,\n",
    "                                            shift_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "pad_l = pad_t = pad_d0 = 0\n",
    "pad_d1 = (window_size[0] - D % window_size[0]) % window_size[0]\n",
    "pad_b = (window_size[1] - H % window_size[1]) % window_size[1]\n",
    "pad_r = (window_size[2] - W % window_size[2]) % window_size[2]\n",
    "x = F.pad(x, (0, 0, pad_l, pad_r, pad_t, pad_b, pad_d0, pad_d1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 6, 6, 6, 96])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
